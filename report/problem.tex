In the last years a vast amount of data has been collected and has been made available to the public in order for everyone to analyze this data and gain knowledge about its structure. One example for such data set is the Hyperlink Graph of the University of Mannheim which has been extracted from the Common Crawl. There are currently two versions of the graph namely the 2012 and 2014 version each covering billions of pages and hyperlinks between those pages. Analyzing this data set may be beneficially for multiple research fields like  search algorithms, SPAM detection or graph analysis algorithms.  
 
The target of this project is to implement algorithms to compute the statistics of importance such as indegree and outdegree distribution, the PageRank, the closeness and the betweenness centrality. Those statistics rank the nodes from different aspects. A major challenge within the implementation part is the scalability of the algorithms. Since the data set consists of millions of nodes, it is strictly required to efficiently compute the different measures. Especially the closeness and betweenness centrality are resource intensive computations. Therefore, those algorithms will be implemented according to Kang et al. [1].

Further, another goal is to analyze the data set with the help of Apache Flink to process huge parts of the data set. Based on the results statements can be made about the structure of the Hyperlink Graph and therefore statements can be formulated about the structure of the World-Wide-Web. Based on the results retrieved from the computations, comparisons will be made to results of other research. Various research groups already analyzed the structure of the web graph [4]. These results can be confirmed or rejected by results retrieved from this project.